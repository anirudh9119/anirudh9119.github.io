<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #07889b; /*#1772d0;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #e37222; /*#f7b733;*/ /*f09228;*/
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    strong {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    heading {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 22px;
    color: #e37222; /*#fc4a1a;*/
    }
    heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 18px;
    }
    papertitle {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    font-weight: 700;
    }
    name {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>  

<title>Anirudh Goyal</title>
  <link href="./_files/css" rel="stylesheet" type="text/css">
  <style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
/*html {
  -webkit-filter: brightness(100%) contrast(100%) grayscale(20%) sepia(10%) !important;
}*/

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>




</head>
  <body><div id="StayFocusd-infobar" style="display: none; top: 2400px;">
    <img src="chrome-extension://laankejkbhbdhmipfmgcngdelahlfoji/common/img/eye_19x19_red.png">
    <span id="StayFocusd-infobar-msg"></span>
    <span id="StayFocusd-infobar-links">
        <a id="StayFocusd-infobar-never-show">hide forever</a>&nbsp;&nbsp;|&nbsp;&nbsp;
        <a id="StayFocusd-infobar-hide">hide once</a>
    </span>
</div>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
        <name>Anirudh Goyal</name><br>
        anirudhgoyal9119 at gmail dot com
        </p>
        <p>I am a graduate student in CS at <a href="www.umontreal.ca/">University of Montreal</a>. I am a part
        of <a href="http://mila.umontreal.ca">MILA</a>, advised by <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/
            ">Prof. Yoshua Bengio</a>
        </p>
        <p>
        Before graduate school, I received a Bachelors in Computer Science at <a href="http://iiit.ac.in">IIIT Hyderabad</a>, where I worked on several research projects at <a href="http://cvit.iiit.ac.in">CVIT</a> under <a href="https://faculty.iiit.ac.in/~jawahar/
            ">Prof. C.V Jawahar</a>. I have also spent time at <a href="https://www.google.com/intl/en/about/">Google</a>.
        </p>
        <p align="center">
<a href="https://scholar.google.com/citations?user=krrh6OUAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
<a href="http://www.github.com/anirudh9119/"> GitHub </a>
        </p>
        </td>
        <td  width=250 height=250>
        <img src="./files/img.jpg"  width=250 height=250>
        </td>
      </tr>
  </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td>
            <heading>News</heading>
            <ul>
              <li> At <a href="https://2018.icml.cc">ICML 2018</a>, I organized a workshop on efficient credit assignment. <a href="https://sites.google.com/view/creditassignmentindlanddrl/home">Efficient Credit Assignment in Deep Learning and Deep Reinforcement Learning </a>.</li>
              <li> At <a href="https://2017.icml.cc">ICML 2017</a>, I organized a workshop <a href="https://sites.google.com/view/icml-reproducibility-workshop/home">Workshop on Reproducibility in Machine Learning</a>.</li>
            </ul>
        </td></tr>

        <tr><td>
            <heading>Invited Talks and Lectures</heading>
            <ul>
              <li> In April 2018, I gave an invited talk at <a href="http://dalimeeting.org/dali2018//program"</a> DALI'2018 </li>
              <li> In April 2018, I gave an invited talk at <a href="https://www.ucl.ac.uk/"</a> UCL Machine learning Department </li>
              <li> In July 2016, I gave a talk on my work Professor Forcing: A new way of training RNN's<a href="https://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks</a> (slides <a href="files/professor_forcing.pdf"> here </a> ). </li>
            </ul>
        </td></tr>

      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
          These days, most of my time goes in thinking how brains do credit assignment through time. 
          For RL problems, I'd like to figure out a method that estimates the gradient of the reward with respect
          to the action probabilities in a way that mimics some of the fundamental properties of
          backprop. Backprop works by *composing* local estimates of effects (Jacobians).
          In general I'm interested in unsupervised exploration, and training generative models! :-)

          </p>

        </td>
      </tr>
     </tbody></table>

     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <heading>Publications</heading> <br><br>
          <!-- -->
          <heading2><i>Preprints</i></heading2><br><br>

            <div onmouseover="document.getElementById('rims').style.display = 'block';"
                onmouseout="document.getElementById('rims').style.display='none';">
              <a href="https://arxiv.org/abs/1909.10893">
                <papertitle> Recurrent Independent Mechanisms </papertitle></a><br>
              <i>Anirudh Goyal</i>,
              <a href="https://scholar.google.co.uk/citations?user=BFzFy1YAAAAJ&hl=en&oi=ao">Alex Lamb</a>,
              <a href="https://scholar.google.com/citations?user=Rkr2uT8AAAAJ&hl=en/"> Jordan Hoffmann </a>,
              <a href="https://scholar.google.co.uk/citations?hl=en&user=ixp-vqMAAAAJ"> Shagun Sodhani </a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://www.is.mpg.de/~bs"> Bernhard Sch√∂lkopf</a>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> <br>
              <a href="https://arxiv.org/abs/1909.10893">arXiv</a>  / <a href="https://github.com/anirudh9119/RIMs">code</a>
            </div>
            <div id="rims" style="display:none">  
              </div><br>
            
             <div onmouseover="document.getElementById('nims').style.display = 'block';"
                onmouseout="document.getElementById('nims').style.display='none';">
              <a href="https://arxiv.org/abs/1910.01075">
                <papertitle> Learning Neural Causal Models from unknown Interventions </papertitle></a><br>
              <a href="https://nke001.github.io/"> Nan Rosemary Ke </a>,
              <a href="http://mila.umontreal.ca/en/person/olexa-bilaniuk/?lang=en"> Olexa Bilaniuk </a>,
              <i>Anirudh Goyal</i>,
              <a href="http://www.is.mpg.de/~sbauer"> Stefan Bauer </a>,
              <a href="https://www.is.mpg.de/~bs"> Bernhard Sch√∂lkopf</a>,
              <a href="https://www.cs.colorado.edu/~mozer/"> Michael C. Mozer</a>,
              <a href="http://info.usherbrooke.ca/hlarochelle/index_en.html"> Hugo Larochelle</a>,
              <a href="https://sites.google.com/view/christopher-pal"> Chris Pal </a>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> <br>
              <a href="https://arxiv.org/abs/1910.01075">arXiv</a>  / <a href="https://github.com/nke001/causal_learning_unknown_interventions">code</a>
            </div>
            <div id="nims" style="display:none">  
              </div><br>
             
            <div onmouseover="document.getElementById('topk').style.display = 'block';"
                onmouseout="document.getElementById('topk').style.display='none';">
              <a href="https://arxiv.org/abs/2002.06224">
                <papertitle> Top-K Training of GANs: Improving Generators by Making Critics Less Critical </papertitle></a><br>
              <a href="http://www.cs.toronto.edu/~sinhasam/"> Samarth Sinha </a>,
              <i>Anirudh Goyal</i>,
              <a href="http://colinraffel.com/"> Colin Raffel </a>,
              <a href="https://www.augustusodena.com/">  Augustus Odena </a> <br>
              <a href="https://arxiv.org/abs/2002.06224">arXiv</a>  
            </div>
            <div id="topk" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('megs').style.display = 'block';"
                onmouseout="document.getElementById('megs').style.display='none';">
              <a href="https://arxiv.org/abs/1901.08508">
                <papertitle> Maximum Entropy Generators for Energy-Based Models </papertitle></a><br>
              <a href="https://ritheshkumar.com/"> Rithesh Kumar </a>,
              <a href="https://scholar.google.co.uk/citations?user=O7MZStwAAAAJ&hl=en&oi=ao">  Sherjil Ozair </a>,
              <i>Anirudh Goyal</i>,
	      <a href="https://mila.quebec/en/person/aaron-courville/"> Aaron Courville </a> <br>
	      <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> <br>
              <a href="https://arxiv.org/abs/1901.08508">arXiv</a> /  <a href="https://github.com/ritheshkumar95/energy_based_generative_models"> code</a>  
            </div>
            <div id="megs" style="display:none">  
              </div><br>

     <heading2><i>2020</i></heading2><br><br>
             <div onmouseover="document.getElementById('brims').style.display = 'block';"
                onmouseout="document.getElementById('brims').style.display='none';">
              <a href="">
              <papertitle> Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules </papertitle></a><br>              
              <a href="https://sarthmit.github.io/"> Sarthak Mittal </a>,
              <a href="https://scholar.google.co.uk/citations?user=BFzFy1YAAAAJ&hl=en&oi=ao">Alex Lamb</a>,
              <i>Anirudh Goyal</i>,
              <a href="https://www.doc.ic.ac.uk/~mpsha/"> Murray Shanahan </a>,
              <a href="http://dms.umontreal.ca/~lajoie/"> Guillaume Lajoie </a>,
              <a href="https://www.cs.colorado.edu/~mozer/"> Michael C. Mozer</a>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2020  <br>
              <a href="">arXiv (coming soon!) </a> 
            </div>
            <div id="brims" style="display:none">  
              </div><br>
             
            <div onmouseover="document.getElementById('small-gan').style.display = 'block';"
                onmouseout="document.getElementById('small-gan').style.display='none';">
              <a href="https://arxiv.org/abs/1910.13540">
              <papertitle> Small-GAN: Speeding Up GAN Training Using Core-sets  </papertitle></a><br>
              <a href="http://www.cs.toronto.edu/~sinhasam/"> Samarth Sinha </a>,
              <a href="https://sites.google.com/view/hanzhang"> Han Zhang </a>,
              <i>Anirudh Goyal</i>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a>,
              <a href="http://info.usherbrooke.ca/hlarochelle/index_en.html"> Hugo Larochelle</a>,
              <a href="https://www.augustusodena.com/">  Augustus Odena </a> <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1910.13540">arXiv</a> 
            </div>
            <div id="small-gan" style="display:none">  
              </div><br>
            
             <div onmouseover="document.getElementById('icp').style.display = 'block';"
                onmouseout="document.getElementById('icp').style.display='none';">
              <a href="https://arxiv.org/abs/1906.10667">
              <papertitle> Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives  </papertitle></a><br>
              <i>Anirudh Goyal</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Shagun Sodhani</a>,
              <a href="http://www.jbinas.com/"> Jonathan Binas  </a>,
              <a href="https://xbpeng.github.io/"> Xue Bin (Jason) Peng </a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> </br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1906.10667">arXiv</a> 
            </div>
            <div id="icp" style="display:none">  
              </div><br>
             

             <div onmouseover="document.getElementById('vbb').style.display = 'block';"
                onmouseout="document.getElementById('vbb').style.display='none';">
              <a href="https://arxiv.org/abs/2004.11935">
              <papertitle> The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget </papertitle></a><br>
              <i>Anirudh Goyal</i>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a>,
              <a href="http://scholar.google.com/citations?user=eM916YMAAAAJ&hl=en"> Matthew Botvinick </a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> </br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/2004.11935">arXiv</a> 
            </div>
            <div id="vbb" style="display:none">  
              </div><br>
             
             <div onmouseover="document.getElementById('meta-transfer').style.display = 'block';"
                onmouseout="document.getElementById('meta-transfer').style.display='none';">
              <a href="https://arxiv.org/abs/1901.10912">
              <papertitle> A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms </papertitle></a><br>
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a>,
              <a href="https://mila.quebec/en/person/tristan-deleu/"> Tristan Deleu </a>,
              <a href="https://scholar.google.co.uk/citations?user=iH9DuY0AAAAJ&hl=en&oi=ao"> Nasim Rahaman </a>,
              <a href="https://nke001.github.io/"> Nan Rosemary Ke </a>,
              <a href="https://mila.quebec/en/person/sebastien-lachapelle/"> Sebastien Lachapelle </a>,
              <a href="http://mila.umontreal.ca/en/person/olexa-bilaniuk/?lang=en"> Olexa Bilaniuk </a>,
              <i>Anirudh Goyal</i>,
              <a href="https://sites.google.com/view/christopher-pal"> Chris Pal </a> </br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1901.10912">arXiv</a> /  <a href="https://github.com/ec6dde01667145e58de60f864e05a4/CausalOptimizationAnon">code</a> 
            </div>
            <div id="meta-transfer" style="display:none">  
              </div><br>

             <div onmouseover="document.getElementById('aot').style.display = 'block';"
                onmouseout="document.getElementById('aot').style.display='none';">
              <a href="https://arxiv.org/abs/1901.10912">
              <papertitle> Learning the Arrow of Time for Problems in Reinforcement Learning  </papertitle></a><br>
              <a href="https://scholar.google.co.uk/citations?user=iH9DuY0AAAAJ&hl=en&oi=ao"> Nasim Rahaman </a>,
              <a href="http://scholar.google.de/citations?user=ZLTMBaEAAAAJ&hl=en"> Steffen Wolf </a>,
              <i>Anirudh Goyal</i>,
              <a href="https://dblp.uni-trier.de/pers/hd/r/Remme:Roman"> Roman Remme </a>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1901.10912">arXiv</a> /  <a href="https://www.sendspace.com/file/0mx0en">code</a> 
            </div>
            <div id="aot" style="display:none">  
              </div><br>

     <heading2><i>2019 </i></heading2><br><br>
             
              <div onmouseover="document.getElementById('long-term-dynamics').style.display = 'block';"
                onmouseout="document.getElementById('long-term-dynamics').style.display='none';">
              <a href="https://arxiv.org/abs/1903.01599">
              <papertitle> Learning Dynamics Model in Reinforcement Learning by Incorporating the Long Term Future  </papertitle></a><br>
              <a href="https://nke001.github.io/"> Nan Rosemary Ke </a>,
              <a href="https://scholar.google.com/citations?user=aWiMKLsAAAAJ&hl=en"> Amanpreet Singh </a>,
              <a href="https://scholar.google.co.uk/citations?user=D4LT5xAAAAAJ&hl=en&oi=ao"> Ahmed Touati </a>,
              <i>Anirudh Goyal</i>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a>,
              <a href="https://www.cc.gatech.edu/~parikh/"> Devi Parikh </a>, 
              <a href="https://www.cc.gatech.edu/~dbatra/"> Dhruv Batra </a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1903.01599">arXiv</a> / <a href="https://github.com/facebookresearch/modeling_long_term_future/">code</a> 
            </div>
            <div id="long-term-dynamics" style="display:none">  
              </div><br>
              

             <div onmouseover="document.getElementById('infobot').style.display = 'block';"
                onmouseout="document.getElementById('infobot').style.display='none';">
              <a href="https://arxiv.org/abs/1901.10902">
              <papertitle> InfoBot: Transfer and Exploration via the Information Bottleneck </papertitle></a><br>
              <i>Anirudh Goyal</i>,
              <a href="https://riashatislam.com/"> Riashat Islam </a>,
              <a href="http://djstrouse.com/"> Daniel Strouse </a>,
              <a href="http://www.zafarali.me/"> Zafarali Ahmed </a>,
              <a href="http://scholar.google.com/citations?user=eM916YMAAAAJ&hl=en"> Matthew Botvinick </a>, 
              <a href="http://info.usherbrooke.ca/hlarochelle/index_en.html"> Hugo Larochelle</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1901.10902"> arXiv</a> 
            </div>
            <div id="infobot" style="display:none">  
            </div><br>

             <div onmouseover="document.getElementById('recall-traces').style.display = 'block';"
                onmouseout="document.getElementById('recall-traces').style.display='none';">
              <a href="https://arxiv.org/abs/1804.00379">
              <papertitle> Recall Traces: Backtracking Models for Efficient Reinforcement Learning </papertitle></a><br>
              <i>Anirudh Goyal</i>,
              <a href="https://scholar.google.co.uk/citations?user=Q6UMpRYAAAAJ&hl=en&oi=ao"> Philemon Brakel </a>,
              <a href="http://acsweb.ucsd.edu/~wfedus/"> William Fedus </a>,
              <a href="https://mila.quebec/en/person/soumye-singhal/"> Soumye Singhal </a>,
              <a href="http://contrastiveconvergence.net/"> Timothy Lillicrap </a>,
              <a href="http://info.usherbrooke.ca/hlarochelle/index_en.html"> Hugo Larochelle</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://yoshuabengio.org/"> Yoshua Bengio </a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1804.00379"> arXiv</a> / <a href="https://github.com/anirudh9119/rl_adversarial">code</a> 
            </div>
            <div id="recall-traces" style="display:none">  
            </div><br>



      <tr onmouseout="sirfs_stop()" onmouseover="sirfs_start()">
          <td width="35%">
            <div class="one">
                <div class="two" id="sirfs_image" style="opacity: 0;"><img src="./files/sab.png" style="border-style: none" width="250"></div>
                <img src="./files/sab.png" style="border-style: none" width="250">
            </div>
            <script type="text/javascript">
            function sirfs_start() {
              document.getElementById('sirfs_image').style.opacity = "1";
            }
            function sirfs_stop() {
              document.getElementById('sirfs_image').style.opacity = "0";
            }
            sirfs_stop()
            </script>

          </td>
        <td width="65%" valign="top">
        <p>
          <a href="https://arxiv.org/abs/1809.03702" id="SIRFS">
          <papertitle>Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding </papertitle>
          </a>
          <br>
          Rosemary Nan Ke, <strong>Anirudh Goyal</strong>, Olexa Bilaniuk, Jonathan Binas, Michael C. Mozer, Chris Pal, Yoshua Bengio <br>
          <em>Neural Information Processing System (NIPS)</em>, 2018 (Oral Presentation) <br>
        </p>
        <p>
        Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly,
        biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to
        the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly long-term dependencies, but without requiring the biologically implausible backward
        replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.

        </p>
        </td>
      </tr>


        <tr onmouseout="place_stop()" onmouseover="place_start()">
          <td width="35%">
            <div class="one">
                <div class="two" id="place_gif" style="opacity: 0;"><img src="./files/fortified_nets.png" width=250></div>
                <img src="./files/fortified_nets.png" width=250>
            </div>
            <script type="text/javascript">
            function place_start() {
              document.getElementById('place_gif').style.opacity = "1";
            }
   function place_stop() {
              document.getElementById('place_gif').style.opacity = "0";
            }
            place_stop()
            </script>

              </td>
              <td valign="top" width="65%">
                <heading2><i></i></heading2><br>
              <p><a href="">
                <papertitle>Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations</papertitle></a><br>
              Alex Lamb,
              Jonathan Binas, 
              <strong>Anirudh Goyal</strong>,
              Dmitriy Serdyuk,
              Sandeep Subramanian,
              Ioannis Mitliagkas,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
                <a href="https://arxiv.org/abs/1804.02485">arXiv</a>
                /
                <a href="https://github.com/anirudh9119/fortified-networks">Code</a>
              </p><p></p>
              <p>
               Deep networks have achieved impressive results across a variety of important tasks. However a known weakness is a failure to perform well when evaluated on data which differ from the training distribution, even if these differences are very small, as is the case with adversarial examples. We propose Fortified Networks, a simple transformation of existing networks, which fortifies the hidden layers in a deep network by identifying when the hidden states are off of the
               data manifold, and maps these hidden states back to parts of the data manifold where the network performs well. Our principal contribution is to show that fortifying these hidden states improves the robustness of deep networks and our experiments (i) demonstrate improved robustness to standard adversarial attacks in both black-box and white-box threat models; (ii) suggest that our improvements are not primarily due to the gradient masking problem and (iii) show the
               advantage of doing this fortification in the hidden layers instead of the input space.

              </p>
              </td>
            </tr>

        <tr onmouseout="place_stop()" onmouseover="place_start()">
          <td width="25%">
            <script type="text/javascript">
            function place_start() {
              document.getElementById('place_gif').style.opacity = "1";
            }
   function place_stop() {
              document.getElementById('place_gif').style.opacity = "0";
            }
            place_stop()
            </script>

              </td>
              <td valign="top" width="75%">
                <heading2><i></i></heading2><br>
              <p><a href="">
                <papertitle> Generalization of Equilibrium Propagation to Vector Field Dynamics</papertitle></a><br>
              Benjamin Scellier, 
              <strong>Anirudh Goyal</strong>,
              Jonathan Binas,
              Thomas Mesnard,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
                <a href="https://arxiv.org/abs/1808.04873">ICLR'18 Workshop</a>
              </p><p></p>
              <p>
                The biological plausibility of the backpropagation algorithm has long been doubted by neuroscientists. Two major reasons are that neurons would need to send two different types of signal in the forward and backward phases, and that pairs of neurons would need to communicate through symmetric bidirectional connections.
                We present a simple two-phase learning procedure for fixed point recurrent networks that addresses both these issues.
                In our model, neurons perform leaky integration and synaptic weights are updated through a local mechanism.
                Our learning method extends the framework of Equilibrium Propagation to general dynamics, relaxing the requirement of an energy function.
                As a consequence of this generalization, the algorithm does not compute the true gradient of the objective function,
                but rather approximates it at a precision which is proven to be directly related to the degree of symmetry of the feedforward and feedback weights.
                We show experimentally that the intrinsic properties of the system lead to alignment of the feedforward and feedback weights, and that our algorithm optimizes the objective function.
              </p>
              </td>
            </tr>
       



        <tr onmouseout="place_stop()" onmouseover="place_start()">
          <td width="35%">
            <div class="one">
                <div class="two" id="place_gif" style="opacity: 0;"><img src="./files/z_forcing" width=200></div>
                <img src="./files/z_forcing.png" width=200>
            </div>
            <script type="text/javascript">
            function place_start() {
              document.getElementById('place_gif').style.opacity = "1";
            }
   function place_stop() {
              document.getElementById('place_gif').style.opacity = "0";
            }
            place_stop()
            </script>

              </td>
              <td valign="top" width="65%">
                <heading2><i></i></heading2><br>
              <p><a href="">
                <papertitle>Z Forcing: Training Stochastic RNN's</papertitle></a><br>
              <strong>Anirudh Goyal</strong>,
              Alessandro Sordoni,
              Marc-Alexandre C√¥t√©,
              Rosemary Nan Ke,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
              <em>Neural Information Processing System (NIPS)</em>, 2017 <br>
                <a href="">arXiv</a>
                /
                <a href="https://github.com/anirudh9119/zforcing_nips17">code</a>
              </p><p></p>
              <p> 
              We proposed a novel approach to incorporate stochastic latent variables in sequential neural networks. The method builds on recent architectures that use latent variables to condition the recurrent dynamics of the network. We augmented the inference network with an RNN that runs backward through the sequence and added a new auxiliary cost that forces the latent variables to reconstruct the state of  that backward RNN, i.e. predict a summary of future observations.
              </p>
              </td>
            </tr>



        <tr onmouseout="maml_stop()" onmouseover="maml_start()">
          <td width="35%">
            <div class="one">
                <div class="two" id="maml_image" style="opacity: 0;"><img src="./files/vw.png" width=250></div>
                <img src="./files/vw.png" width=250>
            </div>
            <script type="text/javascript">
            function maml_start() {
              document.getElementById('maml_image').style.opacity = "1";
            }
   function maml_stop() {
              document.getElementById('maml_image').style.opacity = "0";
            }
            maml_stop()
            </script>

              </td>
              <td valign="top" width="65%">
              <p><a href="">
                <papertitle>Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net</papertitle></a><br>
              <strong>Anirudh Goyal</strong>,
              Nan Rosemary Ke, 
              <a href="https://ganguli-gang.stanford.edu/surya.html">Surya Ganguli</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/
              ">Yoshua Bengio</a> <br>
              <em>Neural Information Processing System (NIPS)</em>, 2017 <br>
                <a href="https://arxiv.org/abs/1711.02282">arXiv</a>
                /
                <a href="https://github.com/anirudh9119/walkback_nips17">code</a>
              </p><p></p>
              <p> 
               We propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function.

              </p>
              </td>
            </tr>




        <tr onmouseout="ssrl_stop()" onmouseover="ssrl_start()">
          <td width="35%">

                  <heading2><i></i></heading2><br>
            <div class="one">
                <div class="two" id="ssrl_image" style="opacity: 0;"><img src="./files/zoneout.png" width=250></div>
                <img src="./files/zoneout.png" width=250>
            </div>
            <script type="text/javascript">
            function ssrl_start() {
              document.getElementById('ssrl_image').style.opacity = "1";
            }
            function ssrl_stop() {
              document.getElementById('ssrl_image').style.opacity = "0";
            }
            ssrl_stop()
            </script>

              </td>
              <td valign="top" width="65%">
              <p><a href="https://arxiv.org/abs/1606.01305">
                <papertitle>Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations</papertitle></a><br>
              David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke,<strong> Anirudh Goyal</strong>
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
              <a href="https://aaroncourville.wordpress.com/">Aaron Courville</a> <br>
              <a href="www.professeurs.polymtl.ca/christopher.pal/">Chris Pal</a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="https://arxiv.org/abs/1606.01305">arXiv</a>
                /
                <a href="https://github.com/teganmaharaj/zoneout">code</a>
              </p><p></p>
              <p>
              We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. 
              </p>
              </td>
            </tr>


        <tr onmouseout="mpc_stop()" onmouseover="mpc_start()">
          <td width="35%">
            <div class="one">
            </div>
            <script type="text/javascript">
            function mpc_start() {
              document.getElementById('mpc_image').style.opacity = "1";
            }
            function mpc_stop() {
              document.getElementById('mpc_image').style.opacity = "0";
            }
            mpc_stop()
            </script>

              </td>
              <td valign="top" width="65%">
              <p><a href="">
                <papertitle>ACtuAL: Actor-Critic Under Adversarial Learning</papertitle></a><br>
                <strong>Anirudh Goyal</strong>, Nan Rosemary Ke, Alex Lamb, R Devon Hjelm, Chris Pal, <a href="www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>
                <a href="https://arxiv.org/abs/1711.04755">arXiv</a>
                /
                <a href="https://github.com/rizar/actor-critic-public">code</a>
              </p><p></p>
              <p>
              Generative Adversarial Networks (GANs) are a powerful framework for deep generative modeling. Posed as a two-player minimax problem, GANs are typically trained end-to-end on real-valued data and can be used to train a generator of high-dimensional and realistic images. However, a major limitation of GANs is that training relies on passing gradients from the discriminator through the generator via back-propagation. This makes it fundamentally difficult to train GANs with
              discrete data, as generation in this case typically involves a non-differentiable function. These difficulties extend to the reinforcement learning setting when the action space is composed of discrete decisions. We address these issues by reframing the GAN framework so that the generator is no longer trained using gradients through the discriminator, but is instead trained using a learned critic in the actor-critic framework with a Temporal Difference (TD) objective. This
              is a natural fit for sequence modeling and we use it to achieve improvements on language modeling tasks over the standard Teacher-Forcing methods.
              </p>
              </td>
            </tr>

        <tr onmouseout="mpc_stop()" onmouseover="mpc_start()">
          <td width="35%">
            <div class="one">
                <div class="two" id="mpc_image" style="opacity: 0;"><img src="./files/actor_crticic.png" width=250 ></div>
                <img src="./files/actor_crticic.png" width=250 >
            </div>
            <script type="text/javascript">
            function mpc_start() {
              document.getElementById('mpc_image').style.opacity = "1";
            }
            function mpc_stop() {
              document.getElementById('mpc_image').style.opacity = "0";
            }
            mpc_stop()
            </script>

              </td>
              <td valign="top" width="65%">
              <p><a href="">
                <papertitle>An Actor-Critic Algorithm for Sequence Prediction</papertitle></a><br>
                Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, <strong>Anirudh Goyal</strong>, Ryan Lowe, <a href="www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a>,
              <a href="https://aaroncourville.wordpress.com/">Aaron Courville</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>
              <em>International Conference on Learning Representations (ICLR)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="https://arxiv.org/abs/1607.07086">arXiv</a>
                /
                <a href="https://github.com/rizar/actor-critic-public">code</a>
              </p><p></p>
              <p>
              We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network
              </p>
              </td>
            </tr>



        <tr onmouseout="rfgps_stop()" onmouseover="rfgps_start()">
          <td width="35%">
            <div class="one">
                <div class="two" id="rfgps_image" style="opacity: 0;"><img src="./files/pf_nips16.png" width=250 ></div>
                        <img src="./files/pf_nips16.png" width=250>
            </div>
            <script type="text/javascript">
            function rfgps_start() {
              document.getElementById('rfgps_image').style.opacity = "1";
            }
            function rfgps_stop() {
              document.getElementById('rfgps_image').style.opacity = "0";
            }
            rfgps_stop()
            </script>

              </td>
              <td valign="top" width="65%">
                <p><a href="https://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf">
                <papertitle>Professor Forcing: A New Algorithm for Training Recurrent Networks</papertitle></a><br>
              <strong>Anirudh Goyal</strong>, Alex Lamb, Ying Zhang, Saizheng Zhang, 
              <a href="https://aaroncourville.wordpress.com/">Aaron Courville</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
              <em> Neural Information on Processing System(NIPS)</em>, 2016 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="https://arxiv.org/abs/1610.09038">arXiv</a>
                /
                <a href="http://videolectures.net/deeplearning2016_goyal_new_algorithm/">video</a>
                /
                <a href="https://github.com/anirudh9119/LM_GANS">code</a>
              </p><p></p>
              <p>
              The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network‚Äôs own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps.
              </p>
              </td>
            </tr>



      </tbody></table>

    </td>
    </tr>
  </tbody></table>


</body></html>
